Field                                                                                               |  Response
:---------------------------------------------------------------------------------------------------|:---------------
Participation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing:  | Input gathered from Deaf, hard-of-hearing, blind/visually impaired, and neurodivergent communities through pilot testing and advisory groups. Feedback incorporated using NVIDIA’s Riva and NeMo frameworks for speech/translation model fine-tuning.
Bias Metric (If Measured):                                                   |  Word Error Rate (WER) by subgroup; ASL avatar translation accuracy vs. certified interpreters; readability scores for simplified text. Metrics tracked and visualized using NVIDIA NeMo Guardrails and TensorBoard integrations.
(For GPAI Models): Which characteristic (feature) show(s) the greatest difference in performance?: |  ASL avatar translation accuracy shows greatest variance on idiomatic and highly technical medical/legal terms, measured with NVIDIA NeMo evaluation pipelines.
(For GPAI Models): Which feature(s) have have the worst performance overall? | Rendering of specialized medical terminology into ASL avatars; simplification of complex legal/clinical documents.
Measures taken to mitigate against unwanted bias:                                                   |  • NVIDIA TAO Toolkit used to fine-tune on diverse datasets (ASL dialects, simplified text, speech). • User testing across protected classes. • Expert-in-the-loop validation. • Regular audits with NVIDIA NeMo Guardrails to enforce fairness and safety.
(For GPAI Models): If using internal data, description of methods implemented in data acquisition or processing, if any, to address the prevalence of identifiable biases in the training, testing, and validation data: | Internal Cone Health documents were stratified by readability (4th–12th grade). ASL data supplemented with interpreter-annotated videos reflecting multiple signing styles. Data pipelines leveraged NVIDIA Clara for secure preprocessing, with validation metrics computed via NVIDIA NeMo to ensure subgroup fairness. Equitable performance confirmed across diverse protected classes.
(For GPAI Models): Tools used to assess statistical imbalances and highlight patterns that may introduce bias into AI models: | NVIDIA NeMo Guardrails, NVIDIA TAO Toolkit evaluation workflows, plus selective use of IBM AI Fairness 360 for secondary benchmarking.
(For GPAI Models): These datasets, such as Cone Health internal patient education sets, public ASL corpora, and NIH/NLM simplified text, do not represent all demographics. For instance, ASL corpora underrepresent regional/cultural variants, and NIH/NLM sets underrepresent non-native English speakers. Internal Cone Health data is weighted toward >10th grade reading level (~62%). To mitigate these, we apply NVIDIA TAO fine-tuning, NeMo evaluation pipelines, and targeted dataset augmentation sourced from community partners.
***You may remove "[]s" before publishing.  
